{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43280579-d6e9-4094-aee9-44b3a6cc5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3647fbd-8450-4783-acdc-dc943f965eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d601cd-d1bf-49ae-8bd1-5e3bff3bf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ed9fcf-6499-4445-b001-5f4d5afe9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Use glob to find all files starting with \"green_tripdata\" and ending with \".parquet\"\n",
    "file_paths = glob.glob(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*.parquet\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "count = 0\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    count+=1\n",
    "    # Read each Parquet file into a DataFrame and append it to the list\n",
    "    dfs.append(pd.read_parquet(file_path))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "print(f\"There are {count} files\")\n",
    "green_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now, df contains the concatenated data from all Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8d1fc-1ca1-4ae7-9746-2014f91078f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2f289f-5e14-44f6-bddc-2ffa28621c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-21 15:17:29</td>\n",
       "      <td>2018-12-21 15:18:57</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 00:10:16</td>\n",
       "      <td>2019-01-01 00:16:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 00:27:11</td>\n",
       "      <td>2019-01-01 00:31:38</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>189</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 00:46:20</td>\n",
       "      <td>2019-01-01 01:04:54</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.68</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-01 00:19:06</td>\n",
       "      <td>2019-01-01 00:39:43</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82</td>\n",
       "      <td>258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.53</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0       2.0  2018-12-21 15:17:29   2018-12-21 15:18:57                  N   \n",
       "1       2.0  2019-01-01 00:10:16   2019-01-01 00:16:32                  N   \n",
       "2       2.0  2019-01-01 00:27:11   2019-01-01 00:31:38                  N   \n",
       "3       2.0  2019-01-01 00:46:20   2019-01-01 01:04:54                  N   \n",
       "4       2.0  2019-01-01 00:19:06   2019-01-01 00:39:43                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         1.0           264           264              5.0           0.00   \n",
       "1         1.0            97            49              2.0           0.86   \n",
       "2         1.0            49           189              2.0           0.66   \n",
       "3         1.0           189            17              2.0           2.68   \n",
       "4         1.0            82           258              1.0           4.53   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0          3.0    0.5      0.5        0.00           0.0        NaN   \n",
       "1          6.0    0.5      0.5        0.00           0.0        NaN   \n",
       "2          4.5    0.5      0.5        0.00           0.0        NaN   \n",
       "3         13.5    0.5      0.5        2.96           0.0        NaN   \n",
       "4         18.0    0.5      0.5        0.00           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3          4.30           2.0        1.0   \n",
       "1                    0.3          7.30           2.0        1.0   \n",
       "2                    0.3          5.80           1.0        1.0   \n",
       "3                    0.3         19.71           1.0        1.0   \n",
       "4                    0.3         19.30           2.0        1.0   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a0076c-45da-4ad3-a824-0f1595594cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Use glob to find all files starting with \"green_tripdata\" and ending with \".parquet\"\n",
    "file_paths = glob.glob(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata_2020*.parquet\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "count = 0\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    count+=1\n",
    "    # Read each Parquet file into a DataFrame and append it to the list\n",
    "    dfs.append(pd.read_parquet(file_path))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "print(f\"There are {count} files\")\n",
    "yellow_df_2020 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now, df contains the concatenated data from all Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0c7407-478e-473e-a60a-b9950c746c6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 527. MiB for an array with shape (9, 7667792) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Read each Parquet file into a DataFrame and append it to the list\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames into a single DataFrame\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    671\u001b[0m     path,\n\u001b[0;32m    672\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    673\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    674\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    675\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    676\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    677\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    679\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\io\\parquet.py:401\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[1;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m     parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mParquetFile(path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparquet_kwargs)\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parquet_file\u001b[38;5;241m.\u001b[39mto_pandas(columns\u001b[38;5;241m=\u001b[39mcolumns, filters\u001b[38;5;241m=\u001b[39mfilters, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\fastparquet\\api.py:769\u001b[0m, in \u001b[0;36mParquetFile.to_pandas\u001b[1;34m(self, columns, categories, filters, index, row_filter, dtypes)\u001b[0m\n\u001b[0;32m    767\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rg\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mfor\u001b[39;00m rg \u001b[38;5;129;01min\u001b[39;00m rgs)\n\u001b[0;32m    768\u001b[0m     selected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(rgs)  \u001b[38;5;66;03m# just to fill zip, below\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m df, views \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_allocate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_value_metadata:\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\fastparquet\\api.py:803\u001b[0m, in \u001b[0;36mParquetFile.pre_allocate\u001b[1;34m(self, size, columns, categories, index, dtypes)\u001b[0m\n\u001b[0;32m    801\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_categories(categories)\n\u001b[0;32m    802\u001b[0m cats \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcats\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns}\n\u001b[1;32m--> 803\u001b[0m df, arrs \u001b[38;5;241m=\u001b[39m \u001b[43m_pre_allocate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_columns_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m i_no_name \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__index_level_\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_pandas_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\fastparquet\\api.py:1058\u001b[0m, in \u001b[0;36m_pre_allocate\u001b[1;34m(size, columns, categories, index, cs, dt, tz, columns_dtype)\u001b[0m\n\u001b[0;32m   1056\u001b[0m cols\u001b[38;5;241m.\u001b[39mextend(cs)\n\u001b[0;32m   1057\u001b[0m dtypes\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(cs))\n\u001b[1;32m-> 1058\u001b[0m df, views \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mindex_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcolumns_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df, views\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\fastparquet\\dataframe.py:223\u001b[0m, in \u001b[0;36mempty\u001b[1;34m(types, size, cats, cols, index_types, index_names, timezones, columns_dtype)\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m             values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     block\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\n\u001b[0;32m    227\u001b[0m mgr\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 527. MiB for an array with shape (9, 7667792) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Use glob to find all files starting with \"green_tripdata\" and ending with \".parquet\"\n",
    "file_paths = glob.glob(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata_2019*.parquet\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "count = 0\n",
    "# Iterate over each file path\n",
    "for i,file_path in enumerate(file_paths):\n",
    "    count+=1\n",
    "    #process the first half\n",
    "    if i == 6:\n",
    "        break\n",
    "    # Read each Parquet file into a DataFrame and append it to the list\n",
    "    dfs.append(pd.read_parquet(file_path))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "print(f\"There are {count} files\")\n",
    "yellow_df_2019_1 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now, df contains the concatenated data from all Parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d2fc5e-e619-472b-893f-55bb205c279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata_2019-05.parquet\"\n",
    "\n",
    "# Define the months you want to process (January to May)\n",
    "months_to_process = ['01', '02', '03', '04', '05']\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Adjust this value based on your memory constraints and dataset size\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and process data for each month\n",
    "for month in months_to_process:\n",
    "    # Construct the file path for the specific month\n",
    "    month_file_path = file_path.replace(\"2019-05\", f\"2019-{month}\")\n",
    "    \n",
    "    # Read data using Dask DataFrame\n",
    "    ddf = dd.read_parquet(month_file_path)\n",
    "    \n",
    "    # Process data using Dask operations\n",
    "    # (e.g., compute summary statistics, perform groupby operations)\n",
    "    df_chunk = ddf.compute()\n",
    "    \n",
    "    # Append the processed chunk to the list\n",
    "    dfs.append(df_chunk)\n",
    "\n",
    "# Concatenate results from all months\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Further processing and analysis on the final DataFrame\n",
    "# (e.g., compute additional statistics, visualize data distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e5163-c8be-4d31-971f-7720361bc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata_2019-05.parquet\"\n",
    "\n",
    "# Define the months you want to process (January to May)\n",
    "months_to_process = ['06', '07', '08', '09', '10','11','12']\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Adjust this value based on your memory constraints and dataset size\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and process data for each month\n",
    "for month in months_to_process:\n",
    "    # Construct the file path for the specific month\n",
    "    month_file_path = file_path.replace(\"2019-05\", f\"2019-{month}\")\n",
    "    \n",
    "    # Read data using Dask DataFrame\n",
    "    ddf = dd.read_parquet(month_file_path)\n",
    "    \n",
    "    # Process data using Dask operations\n",
    "    # (e.g., compute summary statistics, perform groupby operations)\n",
    "    df_chunk = ddf.compute()\n",
    "    \n",
    "    # Append the processed chunk to the list\n",
    "    dfs.append(df_chunk)\n",
    "\n",
    "# Concatenate results from all months\n",
    "final_df_2 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Further processing and analysis on the final DataFrame\n",
    "# (e.g., compute additional statistics, visualize data distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ed58d3-94b0-4a36-befa-2c19035bfd0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 644. MiB for an array with shape (1, 84399019) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     concatenated_dfs\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Concatenate all chunks into a single DataFrame\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m yellow_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcatenated_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    684\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    483\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m!=\u001b[39m empty_dtype_future:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m==\u001b[39m concat_values\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:135\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_ea \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miuf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;66;03m# GH#39817 cast to object instead of casting bools to numeric\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 644. MiB for an array with shape (1, 84399019) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Adjust this value based on your memory constraints and dataset size\n",
    "\n",
    "# Initialize an empty list to store concatenated chunks\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Concatenate DataFrames in chunks for final_df\n",
    "for chunk_start in range(0, len(final_df), chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(final_df))\n",
    "    chunk = final_df.iloc[chunk_start:chunk_end]\n",
    "    concatenated_dfs.append(chunk)\n",
    "\n",
    "# Concatenate DataFrames in chunks for final_df_2\n",
    "for chunk_start in range(0, len(final_df_2), chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(final_df_2))\n",
    "    chunk = final_df_2.iloc[chunk_start:chunk_end]\n",
    "    concatenated_dfs.append(chunk)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "yellow_df = pd.concat(concatenated_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d840117-485b-40be-891c-8bf2824067a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 67108864 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Iterate over row groups (chunks) of the data from the current file and process each chunk\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_row_groups):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Read a specific row group (chunk) from the Parquet file\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mparquet_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_row_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Convert the table to a pandas DataFrame\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     chunk_df \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pyarrow\\parquet.py:307\u001b[0m, in \u001b[0;36mParquetFile.read_row_group\u001b[1;34m(self, i, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03mRead a single row group from a Parquet file.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    Content of the row group as a table (of columns)\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m column_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_column_indices(\n\u001b[0;32m    306\u001b[0m     columns, use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_row_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pyarrow\\_parquet.pyx:1079\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_row_group\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pyarrow\\_parquet.pyx:1104\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_row_groups\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pyarrow\\error.pxi:116\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 67108864 failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "\n",
    "# Use glob to find all files starting with \"green_tripdata\" and ending with \".parquet\"\n",
    "file_paths = glob.glob(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata_2020*.parquet\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Adjust this value based on your available memory\n",
    "\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    # Initialize an empty list to store chunks of data from the current file\n",
    "    file_chunks = []\n",
    "    \n",
    "    # Open the Parquet file\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Get the number of row groups (chunks) in the Parquet file\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "    \n",
    "    # Iterate over row groups (chunks) of the data from the current file and process each chunk\n",
    "    for i in range(num_row_groups):\n",
    "        # Read a specific row group (chunk) from the Parquet file\n",
    "        table = parquet_file.read_row_group(i)\n",
    "        \n",
    "        # Convert the table to a pandas DataFrame\n",
    "        chunk_df = table.to_pandas()\n",
    "        \n",
    "        # Process each chunk if needed\n",
    "        file_chunks.append(chunk_df)\n",
    "    \n",
    "    # Concatenate all chunks from the current file into a single DataFrame and append it to the list\n",
    "    dfs.append(pd.concat(file_chunks, ignore_index=True))\n",
    "\n",
    "# Concatenate all DataFrames from different files into a single DataFrame\n",
    "yellow_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e7e7aa-853d-4d9c-810a-38b73bc83056",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 832. MiB for an array with shape (1, 109047518) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df_chunk)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Concatenate results from all files\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m yellow_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    381\u001b[0m     objs,\n\u001b[0;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    391\u001b[0m )\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    684\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    483\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m!=\u001b[39m empty_dtype_future:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m==\u001b[39m concat_values\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:135\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_ea \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miuf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;66;03m# GH#39817 cast to object instead of casting bools to numeric\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 832. MiB for an array with shape (1, 109047518) and data type float64"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the file pattern\n",
    "file_pattern = \"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/yellow_tripdata*.parquet\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Adjust this value based on your memory constraints and dataset size\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file path using glob\n",
    "for file_path in glob.glob(file_pattern):\n",
    "    # Check if the file path is a pattern or a single file\n",
    "    # Read data using Dask DataFrame\n",
    "    ddf = dd.read_parquet(file_path)\n",
    "    \n",
    "    # Process data using Dask operations\n",
    "    # (e.g., compute summary statistics, perform groupby operations)\n",
    "    df_chunk = ddf.compute()\n",
    "    \n",
    "    # Append the processed chunk to the list\n",
    "    dfs.append(df_chunk)\n",
    "\n",
    "# Concatenate results from all files\n",
    "yellow_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Further processing and analysis on the final DataFrame\n",
    "# (e.g., compute additional statistics, visualize data distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a7abd2-5a33-4215-ac85-ee5e56c20770",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    671\u001b[0m     path,\n\u001b[0;32m    672\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    673\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    674\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    675\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    676\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    677\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    679\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\io\\parquet.py:394\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[1;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     parquet_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\u001b[38;5;241m.\u001b[39mfs\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# use get_handle only when we are very certain that it is not a directory\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\brainiac\\lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*.parquet'"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aaaeb06-b4d0-4379-acbe-759dcfa0fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_18176\\1908614698.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"my_taxi_postgres_data/yellow_tripdata_2021-01.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-01 00:30:10</td>\n",
       "      <td>2021-01-01 00:36:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-01 00:51:20</td>\n",
       "      <td>2021-01-01 00:52:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>238</td>\n",
       "      <td>151</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-01 00:43:30</td>\n",
       "      <td>2021-01-01 01:11:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>51.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-01 00:15:48</td>\n",
       "      <td>2021-01-01 00:31:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-01-01 00:31:49</td>\n",
       "      <td>2021-01-01 00:48:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.36</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       1.0  2021-01-01 00:30:10   2021-01-01 00:36:12              1.0   \n",
       "1       1.0  2021-01-01 00:51:20   2021-01-01 00:52:19              1.0   \n",
       "2       1.0  2021-01-01 00:43:30   2021-01-01 01:11:06              1.0   \n",
       "3       1.0  2021-01-01 00:15:48   2021-01-01 00:31:01              0.0   \n",
       "4       2.0  2021-01-01 00:31:49   2021-01-01 00:48:21              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           2.10         1.0                  N           142            43   \n",
       "1           0.20         1.0                  N           238           151   \n",
       "2          14.70         1.0                  N           132           165   \n",
       "3          10.60         1.0                  N           138           132   \n",
       "4           4.94         1.0                  N            68            33   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0           2.0          8.0    3.0      0.5        0.00           0.0   \n",
       "1           2.0          3.0    0.5      0.5        0.00           0.0   \n",
       "2           1.0         42.0    0.5      0.5        8.65           0.0   \n",
       "3           1.0         29.0    0.5      0.5        6.05           0.0   \n",
       "4           1.0         16.5    0.5      0.5        4.06           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3         11.80                   2.5  \n",
       "1                    0.3          4.30                   0.0  \n",
       "2                    0.3         51.95                   0.0  \n",
       "3                    0.3         36.35                   0.0  \n",
       "4                    0.3         24.36                   2.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"E:/my-learnings/DataEngineering/DataEngineering-zoomcamp-2024/Week_04_Analytics_Engineering/data/green_tripdata*)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015df94a-0166-45f0-9d31-d32eac10e323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1369765, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361ca42a-ee2f-4fc9-ae55-c930a306dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the -tpep_pickup_datetime & tpep_dropoff_datetime to timestamp\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e8ff89-29fe-4c40-804b-840ace21c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1297419-aba4-488c-807b-8e2c4f194c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5433/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e0b6cf7-2111-4c5c-a119-c56aa292135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x19a6e9fd280>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c349b8-671e-4f04-9f5f-80a0fe16d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an iterator to insert data in chunks\n",
    "df_iter = pd.read_csv('my_taxi_postgres_data/yellow_tripdata_2021-01.csv',iterator=True,chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2440306-c1e1-4799-88cd-4ccc0c44d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.parsers.readers.TextFileReader at 0x19a53720980>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "136f6ec7-b189-4720-8964-4ca61f5669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just like generators we can use the next function to view what record/s will be inserted next\n",
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d69e898-22a4-4382-9c03-e31dd94786dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80956ab-5711-4d50-b460-9f8a8f1591a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e6107a3-ebc2-4b76-a928-b6e945281e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e45e3db-033d-4688-89ec-ea6fb7e07ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data',con=engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d2dd5-7143-49da-9d92-5f13e6477b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6c2135-0de4-466f-a3f0-523e93eda7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count BIGINT, \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" BIGINT, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we need to convert this table into a schema\n",
    "print(pd.io.sql.get_schema(df,name='yellow_taxi_data',con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae7a50f-f995-471b-9442-b9ecc33198c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.25-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading SQLAlchemy-2.0.25-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.1 MB 108.9 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.0/2.1 MB 108.9 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.0/2.1 MB 108.9 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.0/2.1 MB 89.3 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.0/2.1 MB 89.3 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.0/2.1 MB 89.3 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.0/2.1 MB 89.3 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.1/2.1 MB 102.4 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.1/2.1 MB 102.4 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.1/2.1 MB 102.4 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.1/2.1 MB 111.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.1/2.1 MB 111.8 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.1/2.1 MB 106.9 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.1/2.1 MB 106.9 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.1/2.1 MB 106.9 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.1/2.1 MB 106.9 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.1/2.1 MB 111.1 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.1/2.1 MB 111.1 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 0.1/2.1 MB 107.6 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.1/2.1 MB 107.6 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.1/2.1 MB 107.6 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.1/2.1 MB 113.6 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.1/2.1 MB 113.6 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 0.2/2.1 MB 119.9 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 119.9 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 119.9 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 116.5 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 0.2/2.1 MB 116.5 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 0.2/2.1 MB 121.6 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 121.6 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 121.6 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 117.5 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 117.5 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 0.2/2.1 MB 117.5 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 119.7 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 119.7 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 119.7 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 124.5 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 124.5 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.3/2.1 MB 121.9 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.3/2.1 MB 121.9 kB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 0.3/2.1 MB 121.9 kB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 124.4 kB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 124.4 kB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 124.4 kB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 122.0 kB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 125.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 125.8 kB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 125.8 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.3/2.1 MB 127.7 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.3/2.1 MB 127.7 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.3/2.1 MB 127.7 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.3/2.1 MB 125.5 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.3/2.1 MB 125.5 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.4/2.1 MB 127.3 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.4/2.1 MB 127.3 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 0.4/2.1 MB 127.3 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 126.0 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 126.0 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 127.6 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 127.6 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 127.6 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 0.4/2.1 MB 129.1 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 0.4/2.1 MB 129.1 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.4/2.1 MB 128.5 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.4/2.1 MB 128.5 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.4/2.1 MB 128.5 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.4/2.1 MB 129.2 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.4/2.1 MB 129.2 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.1 MB 128.1 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.1 MB 128.1 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 0.5/2.1 MB 128.1 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 129.3 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 129.3 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 131.1 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 131.1 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 131.1 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 128.9 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 128.9 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 0.5/2.1 MB 128.9 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 130.5 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 130.5 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 130.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 130.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 130.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.6/2.1 MB 130.6 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.6/2.1 MB 130.6 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.0 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 131.1 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 131.1 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.4 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 132.4 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.6/2.1 MB 132.0 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.6/2.1 MB 132.0 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.6/2.1 MB 132.0 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.6/2.1 MB 132.3 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.6/2.1 MB 132.3 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.7/2.1 MB 134.4 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.7/2.1 MB 134.4 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 0.7/2.1 MB 134.4 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.1 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.1 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 0.7/2.1 MB 133.9 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 0.8/2.1 MB 135.2 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 0.8/2.1 MB 135.2 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 136.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 136.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 136.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 135.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 135.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 135.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 135.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 0.8/2.1 MB 134.9 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 136.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 136.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 136.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 136.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 134.7 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 134.7 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 135.6 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 135.6 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 134.9 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 134.9 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 134.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 135.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 135.8 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 136.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 136.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 136.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 136.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 0.9/2.1 MB 136.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 0.9/2.1 MB 136.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 0.9/2.1 MB 136.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 1.0/2.1 MB 136.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 1.0/2.1 MB 136.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 1.0/2.1 MB 137.2 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.0/2.1 MB 137.2 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 138.3 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 138.3 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 137.3 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 137.3 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 138.9 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.0/2.1 MB 138.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.0/2.1 MB 138.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.0/2.1 MB 138.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.0/2.1 MB 138.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 138.7 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 139.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 139.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 139.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 139.6 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 1.1/2.1 MB 139.6 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 1.1/2.1 MB 140.2 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 140.2 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 140.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 140.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 141.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 141.1 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.1/2.1 MB 141.1 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 141.7 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 141.7 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 142.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 142.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 142.3 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 142.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 142.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 142.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 143.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 143.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 143.5 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.2/2.1 MB 143.1 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.2/2.1 MB 143.1 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.2/2.1 MB 143.1 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.3/2.1 MB 143.7 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.3/2.1 MB 143.7 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.3/2.1 MB 143.3 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 1.3/2.1 MB 143.3 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.4 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.4 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.4 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.1 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.1 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.0 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 1.3/2.1 MB 144.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.2 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.2 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 1.4/2.1 MB 144.9 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.9 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.9 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 146.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 146.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.6 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.4/2.1 MB 145.6 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.4 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 146.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 146.9 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 146.9 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 146.9 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 146.3 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 147.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 147.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 147.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 1.6/2.1 MB 147.8 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 1.6/2.1 MB 147.8 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 1.6/2.1 MB 147.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 1.6/2.1 MB 147.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 1.6/2.1 MB 148.3 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.6/2.1 MB 148.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.6/2.1 MB 148.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.6/2.1 MB 148.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 1.6/2.1 MB 148.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.6/2.1 MB 148.6 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.6/2.1 MB 149.4 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 1.6/2.1 MB 149.4 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.7/2.1 MB 149.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.7/2.1 MB 149.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.7/2.1 MB 149.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.7/2.1 MB 149.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 1.7/2.1 MB 149.5 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 1.7/2.1 MB 150.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 1.7/2.1 MB 150.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 1.7/2.1 MB 150.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 1.7/2.1 MB 150.6 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 1.7/2.1 MB 150.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 151.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 152.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 152.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 152.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 153.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 153.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 153.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 153.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.9/2.1 MB 153.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.9/2.1 MB 154.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.9/2.1 MB 154.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.9/2.1 MB 154.4 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.9/2.1 MB 154.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.9/2.1 MB 155.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.1 MB 155.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.1 MB 156.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 157.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 157.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 157.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 158.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 158.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 159.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 159.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 160.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 160.2 kB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/293.6 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/293.6 kB 435.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/293.6 kB 330.3 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/293.6 kB 409.6 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/293.6 kB 357.2 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/293.6 kB 357.2 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/293.6 kB 291.5 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/293.6 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/293.6 kB 313.8 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 143.4/293.6 kB 341.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 153.6/293.6 kB 316.5 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 174.1/293.6 kB 338.5 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 194.6/293.6 kB 357.9 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 194.6/293.6 kB 357.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 204.8/293.6 kB 311.3 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 235.5/293.6 kB 320.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 256.0/293.6 kB 334.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 276.5/293.6 kB 340.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 293.6/293.6 kB 342.3 kB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: typing-extensions, greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.0.3 sqlalchemy-2.0.25 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e92186f-5712-4183-b8ea-c74ddb4314b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name='yellow_taxi_data',con=engine,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55226650-663d-4353-b614-d216ffd6efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.97 s\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.to_sql(name='yellow_taxi_data',con=engine,if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945f0102-832c-4383-a4b3-72975b28c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3c48c57-4e60-4ace-b239-20225ded3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_18176\\1365744300.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Chunk of data inserted\n",
      "Whole process took 7.288905620574951 amount of time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    try:        \n",
    "        t0 = time()\n",
    "        df = next(df_iter)\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "        df.to_sql(name='yellow_taxi_data',con=engine,if_exists='append')\n",
    "        t1 = time()\n",
    "        t2 = t1-t0\n",
    "    except StopIteration:\n",
    "        break\n",
    "print(\"Last Chunk of data inserted\")\n",
    "print(f\"Whole process took {t2} amount of time\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d6d9d-55fb-426b-9750-bb9cc34a534b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae0b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
